{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b37950",
   "metadata": {},
   "source": [
    "# CS Project: Graph algorithm walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a0f7b",
   "metadata": {},
   "source": [
    "The main algorithm I have implemented is based on creating a graph to representing a piece of music.\n",
    "\n",
    "As mentioned in the presentation, this algorithm can broken down into two stages: **indexing** and **querying**. For the graph-based algorithm, the **indexing** phase can be described as follows:\n",
    "\n",
    "1. Split each piece into segments; each segment should represent a musical phrase as a listener would hear it\n",
    "2. “Reduce” these segments, removing the least *relevant* notes.\n",
    "3. Repeat step 2. until all segments contain only 1 note\n",
    "4. Create a graph, with nodes being the segments, and edges representing reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e365077",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27fa33",
   "metadata": {},
   "source": [
    "We will use the *music21* library to display the music, but the *mido* library is actually used within the algorithm to  read and analyse the music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21 \n",
    "%reload_ext music21.ipython21.ipExtension\n",
    "import project.visualisation.graph as vis\n",
    "\n",
    "\n",
    "show_mid = music21.converter.parse(\"mid/Twinkle_Twinkle_little_star.mid\")\n",
    "show_mid.insert(0, music21.metadata.Metadata())\n",
    "show_mid.metadata.title = 'Twinkle Twinkle Little Star'\n",
    "show_mid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2c962",
   "metadata": {},
   "source": [
    "### Segmentation \n",
    "\n",
    "This is achieved through the **LBDM** algorithm. Given an input MIDI file, the LBDM produces a *boundary strength* profile. The idea is that boundaries (place where a listener would percieve a different musical phrase) occur at places in the music where the change in pitch and note length are significantly different to the boundaries around it.\n",
    "\n",
    "### Example: Visualising LBDM as a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc876c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mido import MidiFile, MidiTrack \n",
    "example_mid = MidiFile(\"mid/Twinkle_Twinkle_little_star.mid\")\n",
    "melody_track = example_mid.tracks[0]\n",
    "\n",
    "# formatting\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "vis.lbdm_graph(melody_track,example_mid.ticks_per_beat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ea1a6",
   "metadata": {},
   "source": [
    "We can use this series of points to find boundaries in the music. The **segmenter** class creates segments from an input MIDI file. The **LbdmSegmenter** class does this by getting the boundary profile by running the LBDM algorithm, and then splitting the input file at points where the *boundary strength* is above a threshold (in this case 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.algorithms.graph_based.lbdm_segmenter import LbdmSegmenter\n",
    "\n",
    "segmenter = LbdmSegmenter(threshold=0.5)\n",
    "segments = segmenter.create_segments(example_mid,track_index=0)\n",
    "for i, segment in enumerate(segments):\n",
    "    segment.save_as_midi(f\"mid/demo/ttwl_{i}.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ceaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_mid_0 = music21.converter.parse(f\"mid/demo/ttwl_0.mid\")\n",
    "segment_mid_0.insert(0, music21.metadata.Metadata())\n",
    "segment_mid_0.metadata.title = 'Twinkle Twinkle Little Star (Segment 0)'\n",
    "segment_mid_0.show()\n",
    "\n",
    "segment_mid_1 = music21.converter.parse(f\"mid/demo/ttwl_1.mid\")\n",
    "segment_mid_1.insert(0, music21.metadata.Metadata())\n",
    "segment_mid_1.metadata.title = 'Twinkle Twinkle Little Star (Segment 1)'\n",
    "segment_mid_1.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125444b",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce638a4",
   "metadata": {},
   "source": [
    "The next phase of the algorithm is *reduction*. This process simplifies each segment by deleting the least **relevant** notes from them. This process is repeated until each segment only has one note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bdbc6",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttwl_segment_0 = segments[0]\n",
    "\n",
    "weight, ttwl_segment_0_reduced = ttwl_segment_0.reduce_segment()\n",
    "\n",
    "ttwl_segment_0_reduced.save_as_midi(\"mid/demo/ttwl_reduction_1.mid\")\n",
    "\n",
    "red_mid = music21.converter.parse(\"mid/demo/ttwl_reduction_1.mid\")\n",
    "red_mid.insert(0, music21.metadata.Metadata())\n",
    "red_mid.metadata.title = 'Twinkle Twinkle Little Star (Segment 0) (reduced)'\n",
    "\n",
    "segment_mid_0.show()\n",
    "red_mid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf899b",
   "metadata": {},
   "source": [
    "As mentioned above, this process goes on until there's only one note left in the segment. Here's an example with a random segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c069324",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reduction = music21.converter.parse(\"mid/generated/graph/nottingham_graph_chord_combined/ashover13/combined_segment_0.mid\")\n",
    "full_reduction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75c427",
   "metadata": {},
   "source": [
    "## Putting it altogether: a *graph* representation of the music\n",
    "\n",
    "So to recap, we can split the music into several segments using LBDM, and then reduce these segments until they only consist of 1 note. From here we create a *graph* of the segments. When creating a graph in this *indexing* stage, this consists of just connecting each segment to it's reduction. This graph is weighted, with the weights being determined by what notes were deleted in the reduction phase\n",
    "\n",
    "Using the `segment_graph` function, we can create a graph of Twinkle Twinkle little star:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.algorithms.graph_based.segment_graph_based import segment_graph\n",
    "\n",
    "segment_graph(\"mid/Twinkle_Twinkle_little_star.mid\",melody_track=0,\n",
    "              output_folder=\"demo\",chord_track=None,save_combined=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cebd9c",
   "metadata": {},
   "source": [
    "The graph is saved as a `pickle` which is a binary format that python can write and read to very quickly. We can read it and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "graph_path = \"mid/generated/graph/demo/Twinkle_Twinkle_little_star/graph.gpickle\"\n",
    "with open(graph_path,\"rb\") as handle:\n",
    "    graph = pickle.load(handle)\n",
    "\n",
    "graph.draw(path=\"example_output/demo_graph.png\")\n",
    "graph_img = mpimg.imread(\"example_output/demo_graph.png\")\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.imshow(graph_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a89aea",
   "metadata": {},
   "source": [
    "## Querying\n",
    "When querying the graph, we treat the query MIDI file as an entire segment and then compute its reductions. Then, for each reduced segment in the graph, we check whether the query segment reduced to the same thing at any point. If this is the case, we add a 0 cost edge between that segment and the query segment.\n",
    "\n",
    "To determine *similarity* between a query and a music piece (represented as a graph), we take either the average, or minimum distance of the shortest path between the original segments and the non-reduced query segment. In the case there's no common reduction, we add a large penalty to the \"distance\" between the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.algorithms.graph_based.query_graph_based import query_graph\n",
    "with open(\"mid/generated/graph/nottingham_graph_nochords/ashover13/graph.gpickle\",\"rb\") as handle:\n",
    "    graph_2 = pickle.load(handle)\n",
    "rankings = query_graph(f\"mid/demo/ttwl_0.mid\", melody_track=0, use_minimum=False, write_graphs=True, graphs=[graph, graph_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47301803",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_img = mpimg.imread(\"example_output/twinkle_graph_example.png\")\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.imshow(graph_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
